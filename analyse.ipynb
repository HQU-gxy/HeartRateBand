{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read again\n",
    "FILE_NAMES = [\n",
    "    \"red_100Hz_2024-04-01_11-29-27\",\n",
    "    \"red_100Hz_2024-04-01_14-40-56\",\n",
    "    \"red_50Hz_2024-04-02_09-46-52\",\n",
    "]\n",
    "FILE_NAME = Path(FILE_NAMES[2] + \".parquet\")\n",
    "table = pq.read_table(FILE_NAME)\n",
    "# read sample rate from filename\n",
    "sample_rate_str = FILE_NAME.stem.split(\"_\")[1]\n",
    "_hz_idx = sample_rate_str.find(\"Hz\")\n",
    "sample_rate = int(sample_rate_str[:_hz_idx])\n",
    "SAMPLE_RATE = sample_rate\n",
    "SAMPLE_INTERVAL = 1 / SAMPLE_RATE\n",
    "display(f\"Sample rate: {SAMPLE_RATE} Hz\", f\"Sample interval: {SAMPLE_INTERVAL} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = table[\"red\"].to_numpy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# sample rate is 800Hz (1.25ms per sample)\n",
    "xs = np.arange(0, data.shape[0] * 1.25e-3, 1.25e-3)\n",
    "px.line(x=xs, y=data, title=\"Raw data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "THRESHOLD = 1.5e6\n",
    "\n",
    "\n",
    "class Level(Enum):\n",
    "    LOW = auto()\n",
    "    HIGH = auto()\n",
    "\n",
    "\n",
    "Segment = tuple[int, int, Level]\n",
    "\n",
    "\n",
    "def segment_data(data: np.ndarray, threshold: float | int) -> list[Segment]:\n",
    "    last_index = 0\n",
    "    last_state = Level.HIGH if data[0] > threshold else Level.LOW\n",
    "    segments: list[Segment] = []\n",
    "    for i, n in enumerate(data):\n",
    "        if n > threshold:\n",
    "            if last_state == Level.LOW:\n",
    "                segments.append((last_index, i, Level.LOW))\n",
    "                last_index = i\n",
    "                last_state = Level.HIGH\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if last_state == Level.HIGH:\n",
    "                segments.append((last_index, i, Level.HIGH))\n",
    "                last_index = i\n",
    "                last_state = Level.LOW\n",
    "            else:\n",
    "                continue\n",
    "        if i == len(data) - 1:\n",
    "            segments.append((last_index, i, last_state))\n",
    "    return segments\n",
    "\n",
    "segments = segment_data(data, THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_length(segment: Segment) -> int:\n",
    "    return segment[1] - segment[0]\n",
    "\n",
    "segment_lens = [segment_length(segment) for segment in segments]\n",
    "np.percentile(segment_lens, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_segments = [s for s in segments if segment_length(s) > 100]\n",
    "display(real_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high plot as red, low plot as blue\n",
    "for segment in real_segments:\n",
    "    color = \"red\" if segment[2] == Level.HIGH else \"blue\"\n",
    "    plt.axvspan(segment[0] * 1.25e-3, segment[1] * 1.25e-3, color=color, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're only interested in the high segments\n",
    "high_segments_idx = [s for s in real_segments if s[2] == Level.HIGH]\n",
    "display(high_segments_idx)\n",
    "high_segments = [data[s[0]:s[1]] for s in high_segments_idx]\n",
    "\n",
    "# lucky = random.sample(high_segments, 1)[0]\n",
    "# lucky_idx = random.randint(0, len(high_segments) - 1)\n",
    "lucky_idx = 0\n",
    "display(f\"lucky index: {lucky_idx}\")\n",
    "# 2 might be a good one\n",
    "# 1484 : 70_000\n",
    "lucky = high_segments[lucky_idx]\n",
    "# filter out below 1 percentile and above 99 percentile\n",
    "# filtered_lucky = np.clip(lucky, np.percentile(lucky, 1),\n",
    "#                          np.percentile(lucky, 99))\n",
    "# TODO: maybe doing some edge detection\n",
    "# like 1D canny\n",
    "# I don't feel the necessity if DC offset is removed (we have different significant DC offset)\n",
    "xs = np.array(range(len(lucky)))\n",
    "xs_time = xs * SAMPLE_INTERVAL\n",
    "# px.line(y=lucky, x=xs).show()\n",
    "trace = go.Scatter(x=xs, y=lucky, mode=\"lines\")\n",
    "trace_time = go.Scatter(x=xs_time, y=lucky, mode=\"lines\")\n",
    "fig = go.Figure(data=[trace_time, trace])\n",
    "# https://community.plotly.com/t/can-plotly-support-2-x-axis-and-2-y-axis-in-one-graph/38303/2\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Sample Index\"),\n",
    "    yaxis=dict(title=\"Red LED Reading (ADC Value)\"),\n",
    "    xaxis2=dict(title=\"Time (s)\", overlaying=\"x\", side=\"top\"),\n",
    ")\n",
    "fig.data[0].update(xaxis=\"x2\", yaxis=\"y\", line=dict(color=\"rgba(0,0,0,0)\")) # type: ignore\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "workable_data:Optional[np.ndarray] = lucky\n",
    "# if FILE_NAME.stem == \"red_100Hz_2024-04-01_11-29-27\":\n",
    "#     if lucky_idx == 1:\n",
    "#         workable_data = lucky[4299:-100]\n",
    "#     if lucky_idx == 2:\n",
    "#         workable_data = lucky[765:-50]\n",
    "#     if lucky_idx == 6:\n",
    "#         workable_data = lucky[1678:-200]\n",
    "\n",
    "xs_time = np.array(range(len(workable_data))) * SAMPLE_INTERVAL # type: ignore\n",
    "px.line(y=workable_data, x=xs_time).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heartpy as hp\n",
    "from scipy.signal import butter, detrend, filtfilt, iirnotch, savgol_filter, wiener, sosfilt, sosfiltfilt, freqz, sosfreqz, ellip\n",
    "from scipy.io import loadmat\n",
    "from heartpy import filter_signal\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.ellip.html\n",
    "\n",
    "mat = loadmat(\"HR_filter_ba.2.50Hz.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2 = mat[\"b\"].flatten()\n",
    "a_2 = mat[\"a\"].flatten()\n",
    "display({\n",
    "    \"b\": b_2,\n",
    "    \"a\": a_2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_bp_2 = butter(1, [0.8, 5], btype=\"band\", fs=SAMPLE_RATE, output=\"sos\")\n",
    "b_s_2, a_s_2 = butter(1, [0.8, 5], btype=\"band\", fs=SAMPLE_RATE, output=\"ba\")\n",
    "filtered_scipy = sosfiltfilt(scipy_bp_2, workable_data)\n",
    "\n",
    "display(f\"scipy: {b_s_2.shape}, {a_s_2.shape}\")\n",
    "display(f\"matlab 2nd order: {b_2.shape}, {a_2.shape}\")\n",
    "# in scipy 2nd order is the 4th order in matlab\n",
    "\n",
    "worN = 4000\n",
    "\n",
    "w, h = sosfreqz(scipy_bp_2, worN=worN)\n",
    "w_2, h_2 = freqz(b_2, a_2, worN=worN)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_title(\"Digital filter frequency response\")\n",
    "ax1.set_ylabel(\"Amplitude (ratio)\")\n",
    "ax1.set_xlabel(\"Frequency (Hz)\")\n",
    "ax1.grid()\n",
    "ax1.set_xlim([0, 10])\n",
    "\n",
    "ax1.plot(0.5 * SAMPLE_RATE * w / np.pi, np.abs(h), label=\"scipy (2nd order)\")\n",
    "ax1.plot(0.5 * SAMPLE_RATE * w_2 / np.pi,\n",
    "         np.abs(h_2),\n",
    "         label=\"matlab (2nd order) ellip\")\n",
    "\n",
    "# some how the matlab filter is significantly worse than scipy\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4Hz to 100Hz\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/examples/1_regular_PPG/Analysing_a_PPG_signal.ipynb\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/examples/5_noisy_ECG/Analysing_Noisy_ECG.ipynb\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/docs/algorithmfunctioning.rst\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/docs/heartrateanalysis.rst\n",
    "\n",
    "# remove_baseline_wander is just a notch filter applied to low frequency (to remove DC offset)\n",
    "# notch filter to remove DC offset\n",
    "# enhance_ecg_peaks is useless\n",
    "# the high pass/low pass/band pass filter here are all butterworth filter\n",
    "\n",
    "# We will use the bandpass variant.\n",
    "# we filter out frequencies below 0.8Hz (<= 48 bpm) (bpm = 60 x Hz)\n",
    "# and above 3Hz (>= 180 bpm)\n",
    "# Second-order sections (SOS) matrix and gain values (G) from MATLAB\n",
    "\n",
    "# by default it only has 2nd order filter\n",
    "\n",
    "filtered_mat = filtfilt(b_2, a_2, workable_data)\n",
    "filtered_scipy = sosfiltfilt(scipy_bp_2, workable_data)\n",
    "\n",
    "# drop the rediculously high values\n",
    "# I'm not sure about the value range\n",
    "filtered_scipy = np.clip(filtered_scipy, -255, 255 - 1)\n",
    "filtered_mat = np.clip(filtered_mat, -255, 255 - 1)\n",
    "\n",
    "trace_bp_matlab = go.Scatter(x=xs_time,\n",
    "                               y=filtered_mat,\n",
    "                               mode=\"lines\",\n",
    "                               name=\"Bandpass Filtered (MATLAB)\")\n",
    "trace_bp = go.Scatter(x=xs_time,\n",
    "                      y=filtered_scipy,\n",
    "                      mode=\"lines\",\n",
    "                      name=\"Bandpass Filtered (Scipy)\")\n",
    "fig = go.Figure(data=[trace_bp, trace_bp_matlab])\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time (s)\"),\n",
    "    yaxis=dict(title=\"Red LED Reading (ADC Value)\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "number = Union[int, float]\n",
    "NDArray = np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from jaxtyping import Int, Float, Bool\n",
    "from typeguard import typechecked\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "IntArray1D = Int[NDArray, \"...\"]\n",
    "\n",
    "\n",
    "# https://leetcode.cn/problems/sliding-window-median\n",
    "# https://ipython-books.github.io/47-implementing-an-efficient-rolling-average-algorithm-with-stride-tricks/\n",
    "# https://aman.ai/code/sliding-window/\n",
    "# https://oi-wiki.org/ds/monotonous-queue/\n",
    "\n",
    "\n",
    "# np.pad(input, (size_before, size_after), mode=\"edge\")\n",
    "# https://github.com/scipy/scipy/blob/2ecac3e596fdb458c85000e7707a8f5f46926621/scipy/ndimage/src/ni_support.c#L222\n",
    "@typechecked\n",
    "def extend_input(input: NDArray, size_before: int,\n",
    "                 size_after: int) -> NDArray:\n",
    "    \"\"\"\n",
    "    abcd -> abcdcba | abcd | dcbabcd\n",
    "    \"\"\"\n",
    "    line_len = len(input)\n",
    "    before_size_diff = line_len - size_before\n",
    "    # [::-1] is python way to reverse (I prefer use `reversed` though)\n",
    "\n",
    "    if size_before != 0:\n",
    "        before = input[:size_before][::-1]\n",
    "        if before_size_diff < 0:\n",
    "            sz = abs(before_size_diff)\n",
    "            before = np.concatenate([before, input[:sz][::-1]])\n",
    "    else:\n",
    "        before = np.array([])\n",
    "\n",
    "    if size_after != 0:\n",
    "        after_size_diff = line_len - size_after\n",
    "        after = input[-size_after:][::-1]\n",
    "        if after_size_diff < 0:\n",
    "            sz = abs(after_size_diff)\n",
    "            after = np.concatenate([after, input[:sz][::-1]])\n",
    "    else:\n",
    "        after = np.array([])\n",
    "\n",
    "    return np.concatenate([before, input, after])\n",
    "\n",
    "\n",
    "def rolling_mean(input: NDArray, window_size: int) -> Tuple[NDArray, number]:\n",
    "    \"\"\"\n",
    "    input: 1D array\n",
    "    window_size: window size\n",
    "    \"\"\"\n",
    "    assert window_size > 0, \"Window size must be greater than 0\"\n",
    "    size_1 = int(window_size / 2)\n",
    "    size_2 = window_size - size_1 - 1\n",
    "    padded = extend_input(input, size_1, size_2)\n",
    "    var_summation = np.sum(padded[:window_size])\n",
    "    output = np.zeros_like(input)\n",
    "    div = var_summation / window_size\n",
    "    output[0] = div\n",
    "\n",
    "    summation = div\n",
    "    # no idea how these crazy size aligns\n",
    "    for i in range(window_size, len(padded)):\n",
    "        var_summation += padded[i]\n",
    "        var_summation -= padded[i - window_size]\n",
    "        div = var_summation / window_size\n",
    "        summation += div\n",
    "        output[i - window_size + 1] = div\n",
    "\n",
    "    approx_mean = summation / len(input)\n",
    "    return output, approx_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestRollingMean(unittest.TestCase):\n",
    "\n",
    "    def test_sz(self):\n",
    "        input_array = np.array([1, 2, 3, 4, 5])\n",
    "        r, a = rolling_mean(input_array, 3)\n",
    "        self.assertEqual(len(r), len(input_array))\n",
    "    \n",
    "    def test_approx_mean(self):\n",
    "        input_array = np.array([1, 2, 3, 4, 5, 52])\n",
    "        r, a = rolling_mean(input_array, 10)\n",
    "        np.testing.assert_almost_equal(a, np.mean(input_array), decimal=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TestExtendInput(unittest.TestCase):\n",
    "\n",
    "    def test_normal_case(self):\n",
    "        \"\"\"Test case where size_before and size_after are less than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([2, 1, 1, 2, 3, 4, 4, 3])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_size_before_larger(self):\n",
    "        \"\"\"Test case where size_before is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4, 4, 3])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_size_after_larger(self):\n",
    "        \"\"\"Test case where size_after is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([2, 1, 1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_both_sizes_larger(self):\n",
    "        \"\"\"Test case where size_before and size_after are larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_empty_array(self):\n",
    "        \"\"\"Test case where the input array is empty\"\"\"\n",
    "        input_array = np.array([])\n",
    "        expected_output = np.array([])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_sizes(self):\n",
    "        \"\"\"Test case where size_before and size_after are zero\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([1, 2, 3, 4])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 0, 0),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_size_before(self):\n",
    "        \"\"\"Test case where size_before is zero and size_after is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 0, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_size_after(self):\n",
    "        \"\"\"Test case where size_before is larger than the array length and size_after is zero\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 0),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_single_element_array(self):\n",
    "        \"\"\"Test case where the input array has a single element\"\"\"\n",
    "        input_array = np.array([1])\n",
    "        expected_output = np.array([1, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 1, 1),\n",
    "                                      expected_output)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def detect_peaks(\n",
    "        hr_data: Float[NDArray, \"sz\"],  # noqa: F821\n",
    "        rol_mean: Float[NDArray, \"sz\"],  # noqa: F821\n",
    "        mean: number,\n",
    "        ma_perc: number) -> Optional[IntArray1D]:\n",
    "    \"\"\"\n",
    "    Detect peaks in heart rate data based on a rolling mean threshold.\n",
    "\n",
    "    This function identifies peaks in the given heart rate data by comparing the data points\n",
    "    against a rolling mean threshold. The threshold is calculated by scaling the rolling mean\n",
    "    with a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hr_data : NDArray\n",
    "        An array containing the heart rate data points.\n",
    "    rol_mean : NDArray\n",
    "        An array containing the rolling mean values corresponding to each data point in hr_data.\n",
    "        The length of rol_mean must be the same as hr_data.\n",
    "    mean : number\n",
    "        The mean value used to calculate the threshold.\n",
    "    ma_perc : number\n",
    "        The percentage used to scale the rolling mean. It must be a value between 0 and 2 (exclusive).\n",
    "        For example, 0.1 means 10% of the peak value.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    NDArray\n",
    "        An array containing the indices of the detected peaks in hr_data.\n",
    "    \"\"\"\n",
    "    assert len(hr_data) == len(\n",
    "        rol_mean), \"Length of input data and rolling mean must be the same\"\n",
    "    assert ma_perc > 0, \"Percentage must be greater than 0\"\n",
    "    assert 0 < ma_perc <= 3, \"Percentage must be between 0 and 2 (0.1 means 10% of the peak value)\"\n",
    "    assert len(hr_data) > 0, \"Input data must not be empty\"\n",
    "\n",
    "    mn = mean * ma_perc\n",
    "    # this comment exists in heartpy already\n",
    "    # might be an alternative way to calculate the scaled rolling mean\n",
    "    #\n",
    "    # r_mean = rol_mean + rol_mean * ma_perc + mn\n",
    "    r_mean = rol_mean + mn\n",
    "\n",
    "    data_ps = np.vstack((np.arange(len(hr_data)), hr_data)).T\n",
    "    # grab the peak based on the scaled rolling mean\n",
    "    peak_ps = data_ps[np.where(data_ps[:, 1] > r_mean)]\n",
    "    if len(peak_ps) == 0:\n",
    "        return None\n",
    "    # not sure about this\n",
    "    last_p = peak_ps[-1]\n",
    "\n",
    "    # remove peaks that are too close to each other (peak should NOT appear\n",
    "    # continuously, like the flat part of square wave)\n",
    "    excl_cont_ps_ = data_ps[np.where(np.diff(peak_ps[:, 0]) > 1)]\n",
    "    if len(excl_cont_ps_) == 0:\n",
    "        return None\n",
    "    # np.diff will return n-1 elements, add one element back (should be optional)\n",
    "    excl_cont_ps = np.vstack([excl_cont_ps_, last_p])\n",
    "\n",
    "    peak_idxs = np.array([], dtype=int)\n",
    "\n",
    "    # find the max y-value in each interval\n",
    "    # put the index of the max y-value into peak_idxs\n",
    "    for p in sliding_window_view(excl_cont_ps[:, 0], 2):\n",
    "        x_0 = int(p[0])\n",
    "        x_1 = int(p[1])\n",
    "        interval_ps = peak_ps[x_0:x_1]\n",
    "        max_idx = np.argmax(interval_ps[:, 1])\n",
    "        origin_idx = int(interval_ps[max_idx][0])\n",
    "        peak_idxs = np.append(peak_idxs, int(origin_idx))\n",
    "\n",
    "    return peak_idxs.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 0.75s (at some sample rate)\n",
    "window_size = int(0.75 / SAMPLE_INTERVAL)\n",
    "r, a = rolling_mean(filtered_mat, window_size)\n",
    "display(\n",
    "    f\"Window size: {window_size}, Approximate mean: {a}, data mean: {np.mean(filtered_mat)}, rolling mean: {np.mean(r)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RRInterval:\n",
    "    # the RR interval in milliseconds\n",
    "    rr: Float[NDArray, \"...\"]\n",
    "    # the index of the peaks in pairs\n",
    "    rr_idxs: Int[NDArray, \"... 2\"]\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def preprocess_peaks_idxs(peaks_idxs: IntArray1D,\n",
    "                          sample_rate: int) -> IntArray1D:\n",
    "    assert sample_rate > 0, \"Sample rate must be greater than 0\"\n",
    "    assert peaks_idxs.ndim == 1, \"Peaks must be a 1D array\"\n",
    "    assert len(peaks_idxs) > 1, \"Peaks must contain at least 2 elements\"\n",
    "    working_peaks = peaks_idxs\n",
    "    if peaks_idxs[0] <= (sample_rate / 1000 * 150):\n",
    "        working_peaks = peaks_idxs[1:]\n",
    "    return working_peaks\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def calc_rr_list(peaks_idxs: IntArray1D, sample_rate: int) -> RRInterval:\n",
    "    assert peaks_idxs.ndim == 1, \"Peaks must be a 1D array\"\n",
    "    assert len(peaks_idxs) > 1, \"Peaks must contain at least 2 elements\"\n",
    "    rr_list = np.diff(peaks_idxs) / sample_rate * 1000\n",
    "    rr_idxs = sliding_window_view(peaks_idxs, 2).astype(int)\n",
    "    return RRInterval(rr_list, rr_idxs)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def calc_bpm_by_len(rr_len: int, hr_sample_len: int,\n",
    "                    sample_rate: number) -> float:\n",
    "    assert rr_len > 0, \"RR interval length must be greater than 0\"\n",
    "    assert hr_sample_len > 0, \"Heart rate sample length must be greater than 0\"\n",
    "    assert sample_rate > 0, \"Sample rate must be greater than 0\"\n",
    "    assert hr_sample_len > rr_len, \"Heart rate sample length must be greater than RR interval length\"\n",
    "    return rr_len / (hr_sample_len / sample_rate) * 60\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def fit_peaks(data: Float[NDArray, \"...\"] | Int[NDArray, \"...\"],\n",
    "              sample_rate: int,\n",
    "              hr_max: int = 190,\n",
    "              hr_min: int = 48,\n",
    "              rrsd_min: float = 0.1,\n",
    "              rrsd_max: float = 1_000) -> Tuple[IntArray1D, RRInterval]:\n",
    "    assert sample_rate > 0, \"Sample rate must be greater than 0\"\n",
    "    assert hr_max > 0, \"Maximum heart rate must be greater than 0\"\n",
    "    assert hr_min > 0, \"Minimum heart rate must be greater than 0\"\n",
    "    assert hr_max > hr_min, \"Maximum heart rate must be greater than minimum heart rate\"\n",
    "    assert len(data) > 0, \"Data must not be empty\"\n",
    "    bl_val = np.min(data)\n",
    "    workable_data = data.copy()\n",
    "    if bl_val < 0:\n",
    "        workable_data = workable_data + abs(bl_val)\n",
    "    sample_interval = 1 / sample_rate\n",
    "    # 0.75ms\n",
    "    window_size = int(0.75 / sample_interval)\n",
    "    r, a = rolling_mean(workable_data, window_size)\n",
    "\n",
    "    # ma_perc_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.5, 2, 3]\n",
    "    ma_perc_candidate: list[float] = [0.04, 0.1, 0.25, 0.5, 0.75, 1.25, 3]\n",
    "    # candidate should be sorted in ascending order\n",
    "    ma_perc_candidate = sorted(ma_perc_candidate)\n",
    "    peak_idxs: Optional[NDArray] = None\n",
    "    rr: Optional[RRInterval] = None\n",
    "    for ma_perc in ma_perc_candidate:\n",
    "        _data = workable_data.copy()\n",
    "        _peak_idxs = detect_peaks(_data, r, a, ma_perc)\n",
    "        if _peak_idxs is None:\n",
    "            warn(f\"No peaks detected with ma_perc: {ma_perc}\")\n",
    "            continue\n",
    "        bpm = calc_bpm_by_len(len(_peak_idxs), len(_data), sample_rate)\n",
    "        if bpm > hr_max or bpm < hr_min:\n",
    "            warn(\n",
    "                f\"Detected heart rate is out of range: {bpm}bpm from ma_perc: {ma_perc}\"\n",
    "            )\n",
    "            continue\n",
    "        _rr = calc_rr_list(_peak_idxs, sample_rate)\n",
    "        rr_std = np.std(_rr.rr)\n",
    "        if rr_std < rrsd_min or rr_std > rrsd_max:\n",
    "            warn(\n",
    "                f\"RR interval standard deviation is too high: {rr_std} from ma_perc: {ma_perc}\"\n",
    "            )\n",
    "            continue\n",
    "        peak_idxs = _peak_idxs\n",
    "        rr = _rr\n",
    "        break\n",
    "\n",
    "    if peak_idxs is None or rr is None:\n",
    "        raise ValueError(\"No valid peak detection found\")\n",
    "    return peak_idxs, rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.percentile(filtered_mat, 0.1)\n",
    "display(f\"Percentile: {pc}\")\n",
    "peaks_idx, rr = fit_peaks(filtered_mat, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def filter_rr_by_time(rr: Float[NDArray, \"sz\"]) -> Bool[NDArray, \"sz\"]:\n",
    "    \"\"\"\n",
    "    Filter out RR interval that is too short or too long\n",
    "    \"\"\"\n",
    "    # define RR range as mean +/- 30%, with a minimum of 300\n",
    "    mean_rr = np.mean(rr)\n",
    "    thirty = mean_rr * 0.3\n",
    "    if thirty <= 300:\n",
    "        upper_thresh = mean_rr + 300\n",
    "        lower_thresh = mean_rr - 300\n",
    "    else:\n",
    "        upper_thresh = mean_rr + thirty\n",
    "        lower_thresh = mean_rr - thirty\n",
    "\n",
    "    return np.logical_and(rr < upper_thresh, rr > lower_thresh)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def filter_rr_by_quotient_filter_iter(\n",
    "        rr: Float[NDArray, \"sz\"],\n",
    "        upper_ratio: float = 1.25,\n",
    "        lower_ratio: float = 0.75) -> Bool[NDArray, \"...\"]:\n",
    "    \"\"\"\n",
    "    applies a quotient filter\n",
    "\n",
    "    Function that applies a quotient filter as described in\n",
    "    \"Piskorki, J., Guzik, P. (2005), Filtering Poincare plots\"\n",
    "    \"\"\"\n",
    "    assert upper_ratio > 0, \"Upper ratio must be greater than 0\"\n",
    "    assert lower_ratio > 0, \"Lower ratio must be greater than 0\"\n",
    "    assert upper_ratio > lower_ratio, \"Upper ratio must be greater than lower ratio\"\n",
    "\n",
    "    def quotient(fst: float, snd: float) -> Bool:\n",
    "        return lower_ratio <= (fst / snd) <= upper_ratio\n",
    "\n",
    "    # https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array\n",
    "    ufunc_quotient = np.frompyfunc(quotient, 2, 1)\n",
    "    window = sliding_window_view(rr, 2)\n",
    "    mask_ = ufunc_quotient(window[:, 0], window[:, 1]).astype(bool)\n",
    "    mask = np.concatenate([mask_, [True]])\n",
    "    return mask\n",
    "\n",
    "\n",
    "def filter_rr_by_quotient_filter(\n",
    "    rr: Float[NDArray, \"sz\"],\n",
    "    iterations: int = 1,\n",
    "    upper_ratio: float = 1.25,\n",
    "    lower_ratio: float = 0.75,\n",
    ") -> Bool[NDArray, \"...\"]:\n",
    "    \"\"\"\n",
    "    applies a quotient filter\n",
    "\n",
    "    Function that applies a quotient filter as described in\n",
    "    \"Piskorki, J., Guzik, P. (2005), Filtering Poincare plots\"\n",
    "    \"\"\"\n",
    "    assert upper_ratio > 0, \"Upper ratio must be greater than 0\"\n",
    "    assert lower_ratio > 0, \"Lower ratio must be greater than 0\"\n",
    "    assert upper_ratio > lower_ratio, \"Upper ratio must be greater than lower ratio\"\n",
    "    assert iterations > 0, \"Iterations must be greater than 0\"\n",
    "    if iterations == 1:\n",
    "        return filter_rr_by_quotient_filter_iter(rr, upper_ratio, lower_ratio)\n",
    "    else:\n",
    "        rr_with_idx = np.vstack([np.arange(len(rr)), rr]).T\n",
    "        # iteration variables\n",
    "        # the index that should be masked\n",
    "        masked_idx = np.array([])\n",
    "        # rr with the original index\n",
    "        # (which will be fewer and fewer by each iteration)\n",
    "        rr_idx_iter = rr_with_idx\n",
    "        for _ in range(iterations):\n",
    "            temp_mask = filter_rr_by_quotient_filter_iter(\n",
    "                rr_idx_iter[:, 1], upper_ratio, lower_ratio)\n",
    "            masked_idx = np.unique(\n",
    "                np.concatenate([masked_idx, rr_idx_iter[~temp_mask][:, 0]]))\n",
    "            rr_idx_iter = rr_idx_iter[temp_mask]\n",
    "        # if index is in masked_idx, then it should be masked (being False in the return array)\n",
    "        return np.logical_not(np.isin(rr_with_idx[:, 0], masked_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate a False to the end since sliding_window_view will always return n-1 elements\n",
    "rr_bool = filter_rr_by_time(rr.rr)\n",
    "peaks_bool = np.concatenate([rr_bool, [False]])\n",
    "rr_bool_quotient = filter_rr_by_quotient_filter(rr.rr, iterations=1)\n",
    "peaks_bool_quotient = np.concatenate([rr_bool_quotient, [False]])\n",
    "rejected_peaks_idx = peaks_idx[~peaks_bool]\n",
    "rejected_peaks_idx_quotient = peaks_idx[~peaks_bool_quotient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_trace = go.Scatter(y=filtered_mat,\n",
    "                          mode=\"lines\",\n",
    "                          name=\"ADC Reading (filtered)\")\n",
    "\n",
    "roll_mean_trace = go.Scatter(y=r,\n",
    "                             mode=\"lines\",\n",
    "                             name=\"moving average\",\n",
    "                             line=dict(color=\"black\", width=0.5))\n",
    "\n",
    "peaks_trace = go.Scatter(\n",
    "    x=peaks_idx,\n",
    "    y=filtered_mat[peaks_idx],\n",
    "    mode=\"markers\",\n",
    "    name=\"peaks\",\n",
    "    marker=dict(color=\"green\"),\n",
    ")\n",
    "\n",
    "peaks_rejected_trace = go.Scatter(\n",
    "    x=rejected_peaks_idx,\n",
    "    y=filtered_mat[rejected_peaks_idx],\n",
    "    mode=\"markers\",\n",
    "    name=\"rejected peaks (time criteria)\",\n",
    "    marker=dict(color=\"red\"),\n",
    ")\n",
    "\n",
    "peaks_rejected_quotient_trace = go.Scatter(\n",
    "    x=rejected_peaks_idx_quotient,\n",
    "    y=filtered_mat[rejected_peaks_idx_quotient],\n",
    "    mode=\"markers\",\n",
    "    name=\"rejected peaks (quotient)\",\n",
    "    marker=dict(color=\"orange\"),\n",
    ")\n",
    "\n",
    "rejected_both_idx = np.intersect1d(rejected_peaks_idx,\n",
    "                                     rejected_peaks_idx_quotient)\n",
    "peaks_rejected_both_trace = go.Scatter(\n",
    "    x=rejected_both_idx,\n",
    "    y=filtered_mat[rejected_both_idx],\n",
    "    mode=\"markers\",\n",
    "    name=\"rejected peaks (both)\",\n",
    "    marker=dict(color=\"purple\"),\n",
    ")\n",
    "\n",
    "layout = go.Layout(title=\"Plot\",\n",
    "                   xaxis=dict(title=\"Index\"),\n",
    "                   yaxis=dict(title=\"Value\"),\n",
    "                   showlegend=True)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        signal_trace,\n",
    "        roll_mean_trace,\n",
    "        peaks_trace,\n",
    "        peaks_rejected_trace,\n",
    "        peaks_rejected_quotient_trace,\n",
    "        peaks_rejected_both_trace,\n",
    "    ],\n",
    "    layout=layout,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_left_time_crit = rr.rr[rr_bool]\n",
    "rr_left_quotient = rr.rr[rr_bool_quotient]\n",
    "rr_left_both = rr.rr[(rr_bool & rr_bool_quotient)]\n",
    "# the count that is left after filtering\n",
    "display(f\"before filtering: {len(rr.rr)}, time_criteria: {len(rr_left_time_crit)}, quotient: {len(rr_left_quotient)}, both: {len(rr_left_both)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "@dataclass\n",
    "class Poincare:\n",
    "    x_plus: Float[NDArray, \"l\"]\n",
    "    x_minus: Float[NDArray, \"l\"]\n",
    "    x_one: Float[NDArray, \"l\"]\n",
    "    x_two: Float[NDArray, \"l\"]\n",
    "    sd_1: number\n",
    "    sd_2: number\n",
    "    s: number\n",
    "    ratio: number\n",
    "\n",
    "@typechecked\n",
    "def calc_poincare(rr: Float[NDArray, \"sz\"]):\n",
    "    x_plus = np.array([], dtype=float)\n",
    "    x_minus = np.array([], dtype=float)\n",
    "    for pair in sliding_window_view(rr, 2):\n",
    "        x_plus = np.append(x_plus, pair[0])\n",
    "        x_minus = np.append(x_minus, pair[1])\n",
    "    x_one = (x_plus - x_minus) / np.sqrt(2)\n",
    "    x_two = (x_plus + x_minus) / np.sqrt(2)\n",
    "    sd_1 = np.sqrt(np.var(x_one))\n",
    "    sd_2 = np.sqrt(np.var(x_two))\n",
    "    s = np.pi * sd_1 * sd_2\n",
    "    ratio = sd_1 / sd_2\n",
    "    return Poincare(x_plus, x_minus, x_one, x_two, float(sd_1), float(sd_2), float(s), float(ratio))\n",
    "\n",
    "def plot_poincare(poincare: Poincare, title: str=\"Poincare Plot\"):\n",
    "    x_plus = poincare.x_plus\n",
    "    x_minus = poincare.x_minus\n",
    "    sd_1 = poincare.sd_1\n",
    "    sd_2 = poincare.sd_2\n",
    "    rr = go.Scatter(x=x_plus, y=x_minus,  mode='markers', name='RR intervals',\n",
    "                         marker=dict(color=\"grey\", opacity=0.75))\n",
    "    mins = np.min([x_plus, x_minus])\n",
    "    maxs = np.max([x_plus, x_minus])\n",
    "    x_mn = np.mean(x_plus)\n",
    "    y_mn = np.mean(x_minus)\n",
    "    identity_line = np.linspace(mins, maxs, 100)\n",
    "    identity = go.Scatter(x=identity_line, y=identity_line, mode='lines', name='identity line',\n",
    "                      line=dict(color='black', dash='dash'))\n",
    "\n",
    "    def rotate_vec(x:number, y:number, angle:number)->Tuple[number, number]:\n",
    "        '''rotates vector around origin point\n",
    "\n",
    "        Function that takes vector and angle, and rotates around origin point\n",
    "        with given amount of degrees.\n",
    "\n",
    "        Helper function for poincare plotting\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int or float\n",
    "            vector x coordinate\n",
    "\n",
    "        y : int or float\n",
    "            vector y coordinate\n",
    "\n",
    "        angle: int or float\n",
    "            the angle of rotation applied to the vecftor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_rot : float\n",
    "            new x coordinate with rotation applied\n",
    "\n",
    "        y_rot : float\n",
    "            new x coordinate with rotation applied\n",
    "        '''\n",
    "        theta = np.radians(angle)\n",
    "\n",
    "        cs = np.cos(theta)\n",
    "        sn = np.sin(theta)\n",
    "\n",
    "        x_rot = (x * cs) - (y * sn)\n",
    "        y_rot = (x * sn) + (y * cs)\n",
    "\n",
    "        return x_rot, y_rot\n",
    "\n",
    "    # Rotate SD1, SD2 vectors 45 degrees counterclockwise and plot\n",
    "\n",
    "    sd1_x_rot, sd1_y_rot = rotate_vec(0, sd_1, 45)\n",
    "    sd2_x_rot, sd2_y_rot = rotate_vec(0, sd_2, 45)\n",
    "\n",
    "    sd1_line = go.Scatter(x=[np.mean(x_plus), np.mean(x_plus) + sd1_x_rot],\n",
    "                          y=[np.mean(x_minus), np.mean(x_minus) + sd1_y_rot],\n",
    "                          mode='lines', name='SD1', line=dict(color='blue'))\n",
    "\n",
    "    sd2_line = go.Scatter(x=[np.mean(x_plus), np.mean(x_plus) - sd2_x_rot],\n",
    "                          y=[np.mean(x_minus), np.mean(x_minus) + sd2_y_rot],\n",
    "                          mode='lines', name='SD2', line=dict(color='red'))\n",
    "\n",
    "    # Ellipse\n",
    "    rotation_matrix = R.from_euler('z', 45, degrees=True).as_matrix()[0:2, 0:2]\n",
    "    e_xs = sd_2 * np.cos(np.linspace(0, 2 * np.pi, 100))\n",
    "    e_ys = sd_1 * np.sin(np.linspace(0, 2 * np.pi, 100))\n",
    "    rotated = np.dot(rotation_matrix, np.vstack([e_xs, e_ys]))\n",
    "    # make e_xs and e_ys rotate 45 degrees\n",
    "    ellipse = go.Scatter(x=rotated[0] + x_mn,\n",
    "                         y=rotated[1] + y_mn,\n",
    "                         mode='lines', name='Ellipse',\n",
    "                         line=dict(color='black'))\n",
    "    \n",
    "    # Layout\n",
    "    layout = go.Layout(title=title,\n",
    "                       xaxis_title='RRi[n] (ms)',\n",
    "                       yaxis_title='RRi[n+1] (ms)',\n",
    "                       showlegend=True,\n",
    "                       legend=dict(x=1, y=1, bgcolor='rgba(255,255,255,0.5)'),\n",
    "                       xaxis=dict(showline=True, linewidth=2, linecolor='black', mirror=True),\n",
    "                       yaxis=dict(showline=True, linewidth=2, linecolor='black', mirror=True),\n",
    "                       plot_bgcolor='white')\n",
    "    fig = go.Figure(data=[rr, identity, sd1_line, sd2_line, ellipse], layout=layout)\n",
    "    xlim_max = np.percentile(x_plus, 90) + 100\n",
    "    xlim_min = np.percentile(x_plus, 10) - 100\n",
    "    ylim_max = np.percentile(x_minus, 90) + 100\n",
    "    ylim_min = np.percentile(x_minus, 10) - 100\n",
    "    fig.update_layout(xaxis_range=[xlim_min, xlim_max], yaxis_range=[ylim_min, ylim_max])\n",
    "    fig.update_layout(yaxis_scaleanchor=\"x\", width=500, height=500)\n",
    "    return fig\n",
    "    \n",
    "poincare = calc_poincare(rr.rr[rr_bool_quotient])\n",
    "fig = plot_poincare(poincare)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def bpm_by_rr(rr: Float[NDArray, \"...\"]) -> float:\n",
    "    return float(60 / (np.mean(rr)/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "@dataclass\n",
    "class TimeDomainHRV:\n",
    "    bpm: number\n",
    "    # i.e. average of the RR intervals\n",
    "    ibi: number\n",
    "    # NN interval is RR interval\n",
    "    # sdnn (standard deviation of NN intervals)\n",
    "    # Values below 50 ms are considered unhealthy, 50-100 ms indicate\n",
    "    # compromised health, and above 100 ms suggest a healthy individual\n",
    "    sdnn: number\n",
    "    # sdsd (standard deviation of successive differences in interbeat intervals)\n",
    "    sdsd: number\n",
    "    rmssd: number\n",
    "    # Higher pNN50 suggests a more relaxed state\n",
    "    pnn20: number\n",
    "    pnn50: number\n",
    "    # median absolute deviation of RR intervals\n",
    "    mad: number\n",
    "\n",
    "\n",
    "def calc_time_domain_hrv(rr: Float[NDArray, \"...\"]) -> TimeDomainHRV:\n",
    "    bpm = bpm_by_rr(rr)\n",
    "    ibi = float(np.mean(rr))\n",
    "    rr_diff = np.abs(np.diff(rr))\n",
    "    rr_sqdiff = np.power(rr_diff, 2)\n",
    "\n",
    "    sdnn = float(np.std(rr))\n",
    "    sdsd = float(np.std(rr_diff))\n",
    "    rmssd = np.sqrt(np.mean(rr_sqdiff))\n",
    "\n",
    "    nn20 = rr_diff[np.where(rr_diff > 20)]\n",
    "    nn50 = rr_diff[np.where(rr_diff > 50)]\n",
    "    pnn20 = len(nn20) / len(rr_diff)\n",
    "    pnn50 = len(nn50) / len(rr_diff)\n",
    "\n",
    "    def calc_mad(data: Float[NDArray, \"...\"]) -> float:\n",
    "        '''computes median absolute deviation\n",
    "\n",
    "        Function that compute median absolute deviation of data slice\n",
    "        See: https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : 1-dimensional numpy array or list\n",
    "            sequence containing data over which to compute the MAD\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : float\n",
    "            the Median Absolute Deviation as computed\n",
    "        '''\n",
    "        med = np.median(data)\n",
    "        return float(np.median(np.abs(data - med)))\n",
    "\n",
    "    mad = calc_mad(rr)\n",
    "    return TimeDomainHRV(bpm, ibi, sdnn, sdsd, rmssd, \n",
    "                         pnn20, pnn50, mad)\n",
    "\n",
    "\n",
    "time_domain_hrv = calc_time_domain_hrv(rr.rr[(rr_bool & rr_bool_quotient)])\n",
    "\n",
    "pprint(time_domain_hrv)\n",
    "pprint({\"sd1\": poincare.sd_1, \"sd2\": poincare.sd_2, \"s\": poincare.s, \"sd1/sd2\": poincare.ratio})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_freq: whether to calculate frequency domain measures\n",
    "# interp_threshold: the amplitude threshold beyond which will be checked for\n",
    "# clipping. Recommended is to take this as the maximum value of the ADC with\n",
    "# some margin for signal noise\n",
    "# reject_segmentwise: whether to reject segments with more than 30% rejected\n",
    "# beats. By default looks at segments of 10 beats at a time.\n",
    "\n",
    "# clean_rr uses by default quotient-filtering, which is a bit aggressive.\n",
    "# You can set 'iqr' or 'z-score' with the clean_rr_method flag.\n",
    "from typing import Literal\n",
    "\n",
    "CLEAN_RR_METHOD = Literal[\"quotient-filter\", \"iqr\", \"z-score\"]\n",
    "clean_rr_method: CLEAN_RR_METHOD = \"quotient-filter\"\n",
    "working, measures = hp.process(\n",
    "    filtered_mat,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    interp_clipping=False,\n",
    "    clean_rr=False,\n",
    "    clean_rr_method=clean_rr_method,\n",
    ")\n",
    "\n",
    "# Take into consideration that the scale for RMSSD doesn't typically exceed +/-\n",
    "# 130, SDSD doesn't differ by much. This means that even a few incorrectly\n",
    "# detected peaks are already introducing large measurement errors into the output\n",
    "# variables. The algorithm described here is specifically designed to handle noisy\n",
    "# PPG data from cheap sensors. The main design criteria was to minimise the number\n",
    "# of incorrectly placed peaks as to minimise the error introduced into the output\n",
    "# measures.\n",
    "\n",
    "display(measures)\n",
    "hp.plotter(working, measures, figsize=(24, 6), moving_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_breathing(working, measures, figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_poincare(working, measures, figsize=(4, 4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
