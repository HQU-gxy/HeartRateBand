{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read again\n",
    "FILE_NAMES = [\n",
    "    \"red_100Hz_2024-04-01_11-29-27\",\n",
    "    \"red_100Hz_2024-04-01_14-40-56\",\n",
    "    \"red_50Hz_2024-04-02_09-46-52\",\n",
    "]\n",
    "FILE_NAME = Path(FILE_NAMES[2] + \".parquet\")\n",
    "table = pq.read_table(FILE_NAME)\n",
    "# read sample rate from filename\n",
    "sample_rate_str = FILE_NAME.stem.split(\"_\")[1]\n",
    "_hz_idx = sample_rate_str.find(\"Hz\")\n",
    "sample_rate = int(sample_rate_str[:_hz_idx])\n",
    "SAMPLE_RATE = sample_rate\n",
    "SAMPLE_INTERVAL = 1 / SAMPLE_RATE\n",
    "display(f\"Sample rate: {SAMPLE_RATE} Hz\", f\"Sample interval: {SAMPLE_INTERVAL} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = table[\"red\"].to_numpy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample rate is 800Hz (1.25ms per sample)\n",
    "xs = np.arange(0, data.shape[0] * 1.25e-3, 1.25e-3)\n",
    "# set x axis label\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Red LED Reading (ADC Value)\")\n",
    "plt.plot(xs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "THRESHOLD = 1.5e6\n",
    "\n",
    "\n",
    "class Level(Enum):\n",
    "    LOW = auto()\n",
    "    HIGH = auto()\n",
    "\n",
    "\n",
    "Segment = tuple[int, int, Level]\n",
    "\n",
    "\n",
    "def segment_data(data: np.ndarray, threshold: float | int) -> list[Segment]:\n",
    "    last_index = 0\n",
    "    last_state = Level.HIGH if data[0] > threshold else Level.LOW\n",
    "    segments: list[Segment] = []\n",
    "    for i, n in enumerate(data):\n",
    "        if n > threshold:\n",
    "            if last_state == Level.LOW:\n",
    "                segments.append((last_index, i, Level.LOW))\n",
    "                last_index = i\n",
    "                last_state = Level.HIGH\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if last_state == Level.HIGH:\n",
    "                segments.append((last_index, i, Level.HIGH))\n",
    "                last_index = i\n",
    "                last_state = Level.LOW\n",
    "            else:\n",
    "                continue\n",
    "        if i == len(data) - 1:\n",
    "            segments.append((last_index, i, last_state))\n",
    "    return segments\n",
    "\n",
    "segments = segment_data(data, THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_length(segment: Segment) -> int:\n",
    "    return segment[1] - segment[0]\n",
    "\n",
    "segment_lens = [segment_length(segment) for segment in segments]\n",
    "np.percentile(segment_lens, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_segments = [s for s in segments if segment_length(s) > 100]\n",
    "display(real_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high plot as red, low plot as blue\n",
    "for segment in real_segments:\n",
    "    color = \"red\" if segment[2] == Level.HIGH else \"blue\"\n",
    "    plt.axvspan(segment[0] * 1.25e-3, segment[1] * 1.25e-3, color=color, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "# we're only interested in the high segments\n",
    "high_segments_idx = [s for s in real_segments if s[2] == Level.HIGH]\n",
    "display(high_segments_idx)\n",
    "high_segments = [data[s[0]:s[1]] for s in high_segments_idx]\n",
    "\n",
    "# lucky = random.sample(high_segments, 1)[0]\n",
    "# lucky_idx = random.randint(0, len(high_segments) - 1)\n",
    "lucky_idx = 0\n",
    "display(f\"lucky index: {lucky_idx}\")\n",
    "# 2 might be a good one\n",
    "# 1484 : 70_000\n",
    "lucky = high_segments[lucky_idx]\n",
    "# filter out below 1 percentile and above 99 percentile\n",
    "# filtered_lucky = np.clip(lucky, np.percentile(lucky, 1),\n",
    "#                          np.percentile(lucky, 99))\n",
    "# TODO: maybe doing some edge detection\n",
    "# like 1D canny\n",
    "# I don't feel the necessity if DC offset is removed (we have different significant DC offset)\n",
    "xs = np.array(range(len(lucky)))\n",
    "xs_time = xs * SAMPLE_INTERVAL\n",
    "# px.line(y=lucky, x=xs).show()\n",
    "trace = go.Scatter(x=xs, y=lucky, mode=\"lines\")\n",
    "trace_time = go.Scatter(x=xs_time, y=lucky, mode=\"lines\")\n",
    "fig = go.Figure(data=[trace_time, trace])\n",
    "# https://community.plotly.com/t/can-plotly-support-2-x-axis-and-2-y-axis-in-one-graph/38303/2\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Sample Index\"),\n",
    "    yaxis=dict(title=\"Red LED Reading (ADC Value)\"),\n",
    "    xaxis2=dict(title=\"Time (s)\", overlaying=\"x\", side=\"top\"),\n",
    ")\n",
    "fig.data[0].update(xaxis=\"x2\", yaxis=\"y\", line=dict(color=\"rgba(0,0,0,0)\")) # type: ignore\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "workable_data:Optional[np.ndarray] = lucky\n",
    "# if FILE_NAME.stem == \"red_100Hz_2024-04-01_11-29-27\":\n",
    "#     if lucky_idx == 1:\n",
    "#         workable_data = lucky[4299:-100]\n",
    "#     if lucky_idx == 2:\n",
    "#         workable_data = lucky[765:-50]\n",
    "#     if lucky_idx == 6:\n",
    "#         workable_data = lucky[1678:-200]\n",
    "\n",
    "xs_time = np.array(range(len(workable_data))) * SAMPLE_INTERVAL # type: ignore\n",
    "px.line(y=workable_data, x=xs_time).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heartpy as hp\n",
    "from scipy.signal import butter, detrend, filtfilt, iirnotch, savgol_filter, wiener, sosfilt, sosfiltfilt, freqz, sosfreqz, ellip\n",
    "from scipy.io import loadmat\n",
    "from heartpy import filter_signal\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.ellip.html\n",
    "\n",
    "mat = loadmat(\"HR_filter_ba.2.50Hz.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2 = mat[\"b\"].flatten()\n",
    "a_2 = mat[\"a\"].flatten()\n",
    "display({\n",
    "    \"b\": b_2,\n",
    "    \"a\": a_2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy_bp_2 = butter(1, [0.8, 5], btype=\"band\", fs=SAMPLE_RATE, output=\"sos\")\n",
    "b_s_2, a_s_2 = butter(1, [0.8, 5], btype=\"band\", fs=SAMPLE_RATE, output=\"ba\")\n",
    "filtered_scipy = sosfiltfilt(scipy_bp_2, workable_data)\n",
    "\n",
    "display(f\"scipy: {b_s_2.shape}, {a_s_2.shape}\")\n",
    "display(f\"matlab 2nd order: {b_2.shape}, {a_2.shape}\")\n",
    "# in scipy 2nd order is the 4th order in matlab\n",
    "\n",
    "worN = 4000\n",
    "\n",
    "w, h = sosfreqz(scipy_bp_2, worN=worN)\n",
    "w_2, h_2 = freqz(b_2, a_2, worN=worN)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_title(\"Digital filter frequency response\")\n",
    "ax1.set_ylabel(\"Amplitude (ratio)\")\n",
    "ax1.set_xlabel(\"Frequency (Hz)\")\n",
    "ax1.grid()\n",
    "ax1.set_xlim([0, 10])\n",
    "\n",
    "ax1.plot(0.5 * SAMPLE_RATE * w / np.pi, np.abs(h), label=\"scipy (2nd order)\")\n",
    "ax1.plot(0.5 * SAMPLE_RATE * w_2 / np.pi,\n",
    "         np.abs(h_2),\n",
    "         label=\"matlab (2nd order) ellip\")\n",
    "\n",
    "# some how the matlab filter is significantly worse than scipy\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4Hz to 100Hz\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/examples/1_regular_PPG/Analysing_a_PPG_signal.ipynb\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/examples/5_noisy_ECG/Analysing_Noisy_ECG.ipynb\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/docs/algorithmfunctioning.rst\n",
    "# https://github.com/paulvangentcom/heartrate_analysis_python/blob/master/docs/heartrateanalysis.rst\n",
    "\n",
    "# remove_baseline_wander is just a notch filter applied to low frequency (to remove DC offset)\n",
    "# notch filter to remove DC offset\n",
    "# enhance_ecg_peaks is useless\n",
    "# the high pass/low pass/band pass filter here are all butterworth filter\n",
    "\n",
    "# We will use the bandpass variant.\n",
    "# we filter out frequencies below 0.8Hz (<= 48 bpm) (bpm = 60 x Hz)\n",
    "# and above 3Hz (>= 180 bpm)\n",
    "# Second-order sections (SOS) matrix and gain values (G) from MATLAB\n",
    "\n",
    "# by default it only has 2nd order filter\n",
    "\n",
    "filtered_mat = filtfilt(b_2, a_2, workable_data)\n",
    "filtered_scipy = sosfiltfilt(scipy_bp_2, workable_data)\n",
    "\n",
    "# drop the rediculously high values\n",
    "# I'm not sure about the value range\n",
    "filtered_scipy = np.clip(filtered_scipy, -255, 255 - 1)\n",
    "filtered_mat = np.clip(filtered_mat, -255, 255 - 1)\n",
    "\n",
    "trace_bp_matlab = go.Scatter(x=xs_time,\n",
    "                               y=filtered_mat,\n",
    "                               mode=\"lines\",\n",
    "                               name=\"Bandpass Filtered (MATLAB)\")\n",
    "trace_bp = go.Scatter(x=xs_time,\n",
    "                      y=filtered_scipy,\n",
    "                      mode=\"lines\",\n",
    "                      name=\"Bandpass Filtered (Scipy)\")\n",
    "fig = go.Figure(data=[trace_bp, trace_bp_matlab])\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time (s)\"),\n",
    "    yaxis=dict(title=\"Red LED Reading (ADC Value)\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "number = Union[int, float]\n",
    "NDArray = np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from jaxtyping import Int, Float\n",
    "from typeguard import typechecked\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "IntArray1D = Int[NDArray, \"...\"]\n",
    "\n",
    "\n",
    "# https://leetcode.cn/problems/sliding-window-median\n",
    "# https://ipython-books.github.io/47-implementing-an-efficient-rolling-average-algorithm-with-stride-tricks/\n",
    "# https://aman.ai/code/sliding-window/\n",
    "# https://oi-wiki.org/ds/monotonous-queue/\n",
    "\n",
    "\n",
    "# np.pad(input, (size_before, size_after), mode=\"edge\")\n",
    "# https://github.com/scipy/scipy/blob/2ecac3e596fdb458c85000e7707a8f5f46926621/scipy/ndimage/src/ni_support.c#L222\n",
    "@typechecked\n",
    "def extend_input(input: NDArray, size_before: int,\n",
    "                 size_after: int) -> NDArray:\n",
    "    \"\"\"\n",
    "    abcd -> abcdcba | abcd | dcbabcd\n",
    "    \"\"\"\n",
    "    line_len = len(input)\n",
    "    before_size_diff = line_len - size_before\n",
    "    # [::-1] is python way to reverse (I prefer use `reversed` though)\n",
    "\n",
    "    if size_before != 0:\n",
    "        before = input[:size_before][::-1]\n",
    "        if before_size_diff < 0:\n",
    "            sz = abs(before_size_diff)\n",
    "            before = np.concatenate([before, input[:sz][::-1]])\n",
    "    else:\n",
    "        before = np.array([])\n",
    "\n",
    "    if size_after != 0:\n",
    "        after_size_diff = line_len - size_after\n",
    "        after = input[-size_after:][::-1]\n",
    "        if after_size_diff < 0:\n",
    "            sz = abs(after_size_diff)\n",
    "            after = np.concatenate([after, input[:sz][::-1]])\n",
    "    else:\n",
    "        after = np.array([])\n",
    "\n",
    "    return np.concatenate([before, input, after])\n",
    "\n",
    "\n",
    "def rolling_mean(input: NDArray, window_size: int) -> Tuple[NDArray, number]:\n",
    "    \"\"\"\n",
    "    input: 1D array\n",
    "    window_size: window size\n",
    "    \"\"\"\n",
    "    assert window_size > 0, \"Window size must be greater than 0\"\n",
    "    size_1 = int(window_size / 2)\n",
    "    size_2 = window_size - size_1 - 1\n",
    "    padded = extend_input(input, size_1, size_2)\n",
    "    var_summation = np.sum(padded[:window_size])\n",
    "    output = np.zeros_like(input)\n",
    "    div = var_summation / window_size\n",
    "    output[0] = div\n",
    "\n",
    "    summation = div\n",
    "    # no idea how these crazy size aligns\n",
    "    for i in range(window_size, len(padded)):\n",
    "        var_summation += padded[i]\n",
    "        var_summation -= padded[i - window_size]\n",
    "        div = var_summation / window_size\n",
    "        summation += div\n",
    "        output[i - window_size + 1] = div\n",
    "\n",
    "    approx_mean = summation / len(input)\n",
    "    return output, approx_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestRollingMean(unittest.TestCase):\n",
    "\n",
    "    def test_sz(self):\n",
    "        input_array = np.array([1, 2, 3, 4, 5])\n",
    "        r, a = rolling_mean(input_array, 3)\n",
    "        self.assertEqual(len(r), len(input_array))\n",
    "    \n",
    "    def test_approx_mean(self):\n",
    "        input_array = np.array([1, 2, 3, 4, 5, 52])\n",
    "        r, a = rolling_mean(input_array, 10)\n",
    "        np.testing.assert_almost_equal(a, np.mean(input_array), decimal=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TestExtendInput(unittest.TestCase):\n",
    "\n",
    "    def test_normal_case(self):\n",
    "        \"\"\"Test case where size_before and size_after are less than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([2, 1, 1, 2, 3, 4, 4, 3])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_size_before_larger(self):\n",
    "        \"\"\"Test case where size_before is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4, 4, 3])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_size_after_larger(self):\n",
    "        \"\"\"Test case where size_after is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([2, 1, 1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_both_sizes_larger(self):\n",
    "        \"\"\"Test case where size_before and size_after are larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_empty_array(self):\n",
    "        \"\"\"Test case where the input array is empty\"\"\"\n",
    "        input_array = np.array([])\n",
    "        expected_output = np.array([])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 2, 2),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_sizes(self):\n",
    "        \"\"\"Test case where size_before and size_after are zero\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([1, 2, 3, 4])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 0, 0),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_size_before(self):\n",
    "        \"\"\"Test case where size_before is zero and size_after is larger than the array length\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([1, 2, 3, 4, 4, 3, 2, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 0, 5),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_zero_size_after(self):\n",
    "        \"\"\"Test case where size_before is larger than the array length and size_after is zero\"\"\"\n",
    "        input_array = np.array([1, 2, 3, 4])\n",
    "        expected_output = np.array([4, 3, 2, 1, 1, 1, 2, 3, 4])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 5, 0),\n",
    "                                      expected_output)\n",
    "\n",
    "    def test_single_element_array(self):\n",
    "        \"\"\"Test case where the input array has a single element\"\"\"\n",
    "        input_array = np.array([1])\n",
    "        expected_output = np.array([1, 1, 1])\n",
    "        np.testing.assert_array_equal(extend_input(input_array, 1, 1),\n",
    "                                      expected_output)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def detect_peaks(\n",
    "        hr_data: Float[NDArray, \"sz\"],  # noqa: F821\n",
    "        rol_mean: Float[NDArray, \"sz\"],  # noqa: F821\n",
    "        mean: number,\n",
    "        ma_perc: number) -> Optional[IntArray1D]:\n",
    "    \"\"\"\n",
    "    Detect peaks in heart rate data based on a rolling mean threshold.\n",
    "\n",
    "    This function identifies peaks in the given heart rate data by comparing the data points\n",
    "    against a rolling mean threshold. The threshold is calculated by scaling the rolling mean\n",
    "    with a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hr_data : NDArray\n",
    "        An array containing the heart rate data points.\n",
    "    rol_mean : NDArray\n",
    "        An array containing the rolling mean values corresponding to each data point in hr_data.\n",
    "        The length of rol_mean must be the same as hr_data.\n",
    "    mean : number\n",
    "        The mean value used to calculate the threshold.\n",
    "    ma_perc : number\n",
    "        The percentage used to scale the rolling mean. It must be a value between 0 and 2 (exclusive).\n",
    "        For example, 0.1 means 10% of the peak value.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    NDArray\n",
    "        An array containing the indices of the detected peaks in hr_data.\n",
    "    \"\"\"\n",
    "    assert len(hr_data) == len(\n",
    "        rol_mean), \"Length of input data and rolling mean must be the same\"\n",
    "    assert ma_perc > 0, \"Percentage must be greater than 0\"\n",
    "    assert 0 < ma_perc <= 3, \"Percentage must be between 0 and 2 (0.1 means 10% of the peak value)\"\n",
    "    assert len(hr_data) > 0, \"Input data must not be empty\"\n",
    "\n",
    "    mn = mean * ma_perc\n",
    "    # this comment exists in heartpy already\n",
    "    # might be an alternative way to calculate the scaled rolling mean\n",
    "    #\n",
    "    # r_mean = rol_mean + rol_mean * ma_perc + mn\n",
    "    r_mean = rol_mean + mn\n",
    "\n",
    "    data_ps = np.vstack((np.arange(len(hr_data)), hr_data)).T\n",
    "    # grab the peak based on the scaled rolling mean\n",
    "    peak_ps = data_ps[np.where(data_ps[:, 1] > r_mean)]\n",
    "    if len(peak_ps) == 0:\n",
    "        return None\n",
    "    # not sure about this\n",
    "    last_p = peak_ps[-1]\n",
    "\n",
    "    # remove peaks that are too close to each other (peak should NOT appear\n",
    "    # continuously, like the flat part of square wave)\n",
    "    excl_cont_ps_ = data_ps[np.where(np.diff(peak_ps[:, 0]) > 1)]\n",
    "    if len(excl_cont_ps_) == 0:\n",
    "        return None\n",
    "    # np.diff will return n-1 elements, add one element back (should be optional)\n",
    "    excl_cont_ps = np.vstack([excl_cont_ps_, last_p])\n",
    "\n",
    "    peak_idxs = np.array([], dtype=int)\n",
    "\n",
    "    # find the max y-value in each interval\n",
    "    # put the index of the max y-value into peak_idxs\n",
    "    for p in sliding_window_view(excl_cont_ps[:, 0], 2):\n",
    "        x_0 = int(p[0])\n",
    "        x_1 = int(p[1])\n",
    "        interval_ps = peak_ps[x_0:x_1]\n",
    "        max_idx = np.argmax(interval_ps[:, 1])\n",
    "        origin_idx = int(interval_ps[max_idx][0])\n",
    "        peak_idxs = np.append(peak_idxs, int(origin_idx))\n",
    "\n",
    "    return peak_idxs.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need 0.75s (at some sample rate)\n",
    "window_size = int(0.75 / SAMPLE_INTERVAL)\n",
    "r, a = rolling_mean(filtered_mat, window_size)\n",
    "display(\n",
    "    f\"Window size: {window_size}, Approximate mean: {a}, data mean: {np.mean(filtered_mat)}, rolling mean: {np.mean(r)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RRInterval:\n",
    "    rr: Float[NDArray, \"...\"]\n",
    "    rr_idxs: Int[NDArray, \"... 2\"]\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def preprocess_peaks_idxs(peaks_idxs: IntArray1D,\n",
    "                          sample_rate: int) -> IntArray1D:\n",
    "    assert sample_rate > 0, \"Sample rate must be greater than 0\"\n",
    "    assert peaks_idxs.ndim == 1, \"Peaks must be a 1D array\"\n",
    "    assert len(peaks_idxs) > 1, \"Peaks must contain at least 2 elements\"\n",
    "    working_peaks = peaks_idxs\n",
    "    if peaks_idxs[0] <= (sample_rate / 1000 * 150):\n",
    "        working_peaks = peaks_idxs[1:]\n",
    "    return working_peaks\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def calc_rr_list(peaks_idxs: IntArray1D, sample_rate: int) -> RRInterval:\n",
    "    assert peaks_idxs.ndim == 1, \"Peaks must be a 1D array\"\n",
    "    assert len(peaks_idxs) > 1, \"Peaks must contain at least 2 elements\"\n",
    "    rr_list = np.diff(peaks_idxs) / sample_rate * 1000\n",
    "    rr_idxs = sliding_window_view(peaks_idxs, 2).astype(int)\n",
    "    return RRInterval(rr_list, rr_idxs)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def fit_peaks(data: Float[NDArray, \"...\"] | Int[NDArray, \"...\"],\n",
    "              sample_rate: int,\n",
    "              hr_max: int = 190,\n",
    "              hr_min: int = 48,\n",
    "              rrsd_min: float = 0.1,\n",
    "              rrsd_max: float = 1_000) -> Tuple[IntArray1D, RRInterval]:\n",
    "    assert sample_rate > 0, \"Sample rate must be greater than 0\"\n",
    "    assert hr_max > 0, \"Maximum heart rate must be greater than 0\"\n",
    "    assert hr_min > 0, \"Minimum heart rate must be greater than 0\"\n",
    "    assert hr_max > hr_min, \"Maximum heart rate must be greater than minimum heart rate\"\n",
    "    assert len(data) > 0, \"Data must not be empty\"\n",
    "    bl_val = np.min(data)\n",
    "    workable_data = data.copy()\n",
    "    if bl_val < 0:\n",
    "        workable_data = workable_data + abs(bl_val)\n",
    "    sample_interval = 1 / sample_rate\n",
    "    # 0.75ms\n",
    "    window_size = int(0.75 / sample_interval)\n",
    "    r, a = rolling_mean(workable_data, window_size)\n",
    "\n",
    "    # ma_perc_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.5, 2, 3]\n",
    "    ma_perc_candidate: list[float] = [0.05, 0.1, 0.25, 0.5, 0.75, 1.25, 3]\n",
    "    # candidate should be sorted in ascending order\n",
    "    ma_perc_candidate = sorted(ma_perc_candidate)\n",
    "    peak_idxs: Optional[NDArray] = None\n",
    "    rr: Optional[RRInterval] = None\n",
    "    for ma_perc in ma_perc_candidate:\n",
    "        _data = workable_data.copy()\n",
    "        _peak_idxs = detect_peaks(_data, r, a, ma_perc)\n",
    "        if _peak_idxs is None:\n",
    "            warn(f\"No peaks detected with ma_perc: {ma_perc}\")\n",
    "            continue\n",
    "        bpm = len(_peak_idxs) / (len(workable_data) / sample_rate) * 60\n",
    "        if bpm > hr_max or bpm < hr_min:\n",
    "            warn(f\"Detected heart rate is out of range: {bpm}\")\n",
    "            continue\n",
    "        _rr = calc_rr_list(_peak_idxs, sample_rate)\n",
    "        rr_std = np.std(_rr.rr)\n",
    "        if rr_std < rrsd_min or rr_std > rrsd_max:\n",
    "            warn(f\"RR interval standard deviation is too high: {rr_std}\")\n",
    "            continue\n",
    "        peak_idxs = _peak_idxs\n",
    "        rr = _rr\n",
    "        break\n",
    "\n",
    "    if peak_idxs is None or rr is None:\n",
    "        raise ValueError(\"No valid peak detection found\")\n",
    "    return peak_idxs, rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.percentile(filtered_mat, 0.1)\n",
    "display(f\"Percentile: {pc}\")\n",
    "idx_1, rr = fit_peaks(filtered_mat, SAMPLE_RATE)\n",
    "# plt.plot(filtered_mat)\n",
    "# plt.plot(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1, rr = fit_peaks(filtered_mat, SAMPLE_RATE)\n",
    "\n",
    "fig = plt.figure(figsize=(24, 6))\n",
    "plt.plot(filtered_mat)\n",
    "# grey\n",
    "plt.plot(r, \"k\", alpha=0.5)\n",
    "plt.plot(idx_1, filtered_mat[idx_1], \"go\")\n",
    "# plt.xlim([1000, 3000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rr(peak_list: NDArray, sample_rate: int):\n",
    "    #delete first peak if within first 150ms (signal might start mid-beat after peak)\n",
    "    def preprocess(peak_list: NDArray, sample_rate: int) -> NDArray:\n",
    "        if len(peak_list) > 0:\n",
    "            if peak_list[0] <= ((sample_rate / 1000.0) * 150):\n",
    "                peak_list = np.delete(peak_list, 0)\n",
    "        return peak_list\n",
    "\n",
    "    new_peak_list = preprocess(peak_list, sample_rate)\n",
    "    # RR seems in miliseconds\n",
    "    rr_list = (np.diff(new_peak_list) / sample_rate) * 1000.0\n",
    "    rr_indices = sliding_window_view(new_peak_list, 2)\n",
    "    rr_diff = np.abs(np.diff(rr_list))\n",
    "    rr_sqdiff = np.power(rr_diff, 2)\n",
    "    if len(peak_list) > 0:\n",
    "        rrsd = np.std(rr_list)\n",
    "    else:\n",
    "        rrsd = np.nan\n",
    "\n",
    "# check_peak (by rr interval +- 30% w/ min 300ms)\n",
    "# check_rr (quotient_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_freq: whether to calculate frequency domain measures\n",
    "# interp_threshold: the amplitude threshold beyond which will be checked for\n",
    "# clipping. Recommended is to take this as the maximum value of the ADC with\n",
    "# some margin for signal noise\n",
    "# reject_segmentwise: whether to reject segments with more than 30% rejected\n",
    "# beats. By default looks at segments of 10 beats at a time.\n",
    "\n",
    "# clean_rr uses by default quotient-filtering, which is a bit aggressive.\n",
    "# You can set 'iqr' or 'z-score' with the clean_rr_method flag.\n",
    "from typing import Literal\n",
    "\n",
    "CLEAN_RR_METHOD = Literal[\"quotient-filter\", \"iqr\", \"z-score\"]\n",
    "clean_rr_method: CLEAN_RR_METHOD = \"iqr\"\n",
    "working, measures = hp.process(\n",
    "    filtered_mat,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    interp_clipping=False,\n",
    "    clean_rr=False,\n",
    "    clean_rr_method=clean_rr_method,\n",
    ")\n",
    "\n",
    "# Take into consideration that the scale for RMSSD doesn't typically exceed +/-\n",
    "# 130, SDSD doesn't differ by much. This means that even a few incorrectly\n",
    "# detected peaks are already introducing large measurement errors into the output\n",
    "# variables. The algorithm described here is specifically designed to handle noisy\n",
    "# PPG data from cheap sensors. The main design criteria was to minimise the number\n",
    "# of incorrectly placed peaks as to minimise the error introduced into the output\n",
    "# measures.\n",
    "\n",
    "display(measures)\n",
    "hp.plotter(working, measures, figsize=(24, 6), moving_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_breathing(working, measures, figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_poincare(working, measures, figsize=(4, 4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
